# coupling preliminary chemistry data together across runs and with FTICR, for RC2
# steps:
# combine NPOC and TN across runs
# read in FTICR
# merge each dataset with the metadata with dates and times
# further merge each dataset with with the catchment area data from Kyongho
rm(list=ls());graphics.off();
campaign = 'RC2' # define which campaign is the focus, currently only written for RC2
PNNL.dir = "C:/Users/gara009/"
home.dir = "OneDrive - PNNL/Documents - Core Richland and Sequim Lab-Field Team/Data Generation and Files/"
ion.in.dir = paste0(PNNL.dir,home.dir,campaign,"/Ions/02_FormattedData/")
meta.in.dir = "//pnl/projects/SBR_SFA/RC2/03_Temporal_Study/06_Metadata/"
fticr.in.dir = "//pnl/projects/SBR_SFA/RC2/03_Temporal_Study/08_FTICR/Diversity Analyses/"
npoc.tn.in.dir = paste0(PNNL.dir,home.dir,"/Matched_files/",campaign,"/")
out.dir = paste0(PNNL.dir,home.dir,campaign,"/Prelim_Data_Integration/")
npoc.tn.files = list.files(npoc.tn.in.dir)
npoc.tn.combined = numeric()
for (curr.file in npoc.tn.files) {
npoc.tn.combined = rbind(npoc.tn.combined,read.csv(paste0(npoc.tn.in.dir,curr.file),stringsAsFactors = F))
}
npoc.tn.combined$Event_ID = substring(npoc.tn.combined$Sample_ID,first = 1,last = 8) # pull out the unique event 'parent'
str(npoc.tn.combined)
ion.compiled = matrix()
ion.folders = list.files(path = ion.in.dir)
for (curr.folder in ion.folders) {
ion.files = list.files(path = paste0(ion.in.dir,curr.folder))
ion.compiled.within.date = matrix()
for (curr.ion.file in ion.files) {
ion.name = strsplit(x = curr.ion.file,split = "_")[[1]][5]
ion.temp = read.csv(paste0(ion.in.dir,curr.folder,"/",curr.ion.file),stringsAsFactors = F)
ion.temp = ion.temp[-which(is.na(ion.temp$Sample_ID) == T),c('Amount','Sample_ID')]
ion.temp = ion.temp[grep(pattern = campaign,x = ion.temp$Sample_ID),]
colnames(ion.temp)[which(colnames(ion.temp) == 'Amount')] = ion.name
ion.temp[,ion.name] = suppressWarnings(as.numeric(ion.temp[,ion.name]))
ion.means = as.data.frame(tapply(X = ion.temp[,ion.name],INDEX = ion.temp$Sample_ID,FUN = mean))
colnames(ion.means)[1] = ion.name
ion.means$Sample_ID = row.names(ion.means)
row.names(ion.means) = NULL
ion.temp = ion.means
if (ncol(ion.compiled.within.date) == 1) {
ion.compiled.within.date = ion.temp
} else {
ion.compiled.within.date = unique(merge(ion.compiled.within.date,ion.temp,by = 'Sample_ID',all = T))
}
}
if (ncol(ion.compiled) == 1) {
ion.compiled = ion.compiled.within.date
} else {
if (identical(colnames(ion.compiled),colnames(ion.compiled.within.date)) == T) {
ion.compiled = rbind(ion.compiled,ion.compiled.within.date)
print('Good news, column names match')
} else {
print('Error: Column names do not match, must fix')
break()
}
}
}
for (i in which(colnames(ion.compiled) != 'Sample_ID')) {
ion.compiled[,i] = as.numeric(ion.compiled[,i])
}
ion.compiled$Event_ID = substring(ion.compiled$Sample_ID,first = 1,last = 8) # pull out the unique event 'parent'
str(ion.compiled)
head(ion.compiled)
# this is the full set
fticr.div = read.csv(paste0(fticr.in.dir,"RC2_Diversity_Metrics.csv"))
colnames(fticr.div)[which(colnames(fticr.div) == 'X')] = "FTICR_Sample_ID"
fticr.div$Event_ID = substring(fticr.div$FTICR_Sample_ID,first = 1,last = 8) # pull out the unique event 'parent'
head(fticr.div)
# this is the PCA subset
fticr.pca.div = read.csv(paste0(fticr.in.dir,"RC2_PCA_Diversity_Metrics.csv"))
colnames(fticr.pca.div)[which(colnames(fticr.pca.div) == 'X')] = "FTICR_Sample_ID"
head(fticr.pca.div)
# merge the whole fticr with the PCA subset
fticr.div = merge(fticr.div,fticr.pca.div,by="FTICR_Sample_ID")
metadata = read.csv(paste0(meta.in.dir,"RC2 Temporal Study Responses (Responses) - Form Responses 1.csv"),stringsAsFactors = F)
metadata$Event_ID = NA
for (i in 1:nrow(metadata)) {
metadata$Event_ID[i] = paste0("RC2_",paste0(rep(x = 0,(4-nchar(x = metadata$Site_Vial_ID_.4_digit_numeric_code.[i]))),collapse = ""),metadata$Site_Vial_ID_.4_digit_numeric_code.[i])
}
str(metadata)
npoc.tn.meta = merge(metadata,npoc.tn.combined,by="Event_ID")
dim(npoc.tn.meta)
fticr.meta = merge(metadata,fticr.div,by="Event_ID")
dim(fticr.meta)
ion.meta = merge(metadata,ion.compiled,by="Event_ID")
dim(ion.meta)
cat.dat = read.csv(paste0(meta.in.dir,"Rc2_allsites_0717.csv"),stringsAsFactors = F)
cat.dat = cat.dat[which(cat.dat$Study == "Temporal"),c('Study',"Name","TotDASqKM","D50_m")]
cat.dat$Name[grep(cat.dat$Name,pattern = "American")] = "American River"
cat.dat$Name[grep(cat.dat$Name,pattern = "Union Gap")] = "Union Gap"
cat.dat$Name[grep(cat.dat$Name,pattern = "Little Naches")] = "Little Naches"
cat.dat$Name[grep(cat.dat$Name,pattern = "Mabton")] = "Mabton"
cat.dat$Name[grep(cat.dat$Name,pattern = "Kiona")] = "Kiona"
cat.dat$Name[grep(cat.dat$Name,pattern = "Craig Road 1")] = "Naches- Craig Road 1"
cat.dat$Name[grep(cat.dat$Name,pattern = "Craig Road 2")] = "Naches- Craig Road 2"
str(cat.dat)
npoc.tn.cat = merge(npoc.tn.meta,cat.dat,by.x = "Location",by.y = "Name")
npoc.tn.cat$Location[grep(npoc.tn.cat$Location,pattern = "Craig")] = "Naches-Craig"
dim(npoc.tn.cat)
fticr.cat = merge(fticr.meta,cat.dat,by.x = "Location",by.y = "Name")
fticr.cat$Location[grep(fticr.cat$Location,pattern = "Craig")] = "Naches-Craig"
dim(fticr.cat)
ion.cat = merge(ion.meta,cat.dat,by.x = "Location",by.y = "Name")
ion.cat$Location[grep(ion.cat$Location,pattern = "Craig")] = "Naches-Craig"
dim(ion.cat)
npoc.tn.cat$Date_.mm.dd.yyyy. = as.Date(npoc.tn.cat$Date_.mm.dd.yyyy., "%m/%d/%Y")
################
# CAUTION !!! this is pulling out what looks like bad data, but hasn't been formally examined
npoc.tn.cat = npoc.tn.cat[which(npoc.tn.cat$NPOC_after_correcting_for_dilution < 3),]
pdf(paste0(out.dir,"Prelim_NPOC_Time.pdf"))
View(ion.compiled)
View(npoc.tn.combined)
View(fticr.cat)
View(fticr.div)
View(fticr.meta)
View(ion.means)
View(fticr.cat)
View(ion.compiled)
plot(npoc.tn.cat$NPOC_after_correcting_for_dilution ~ npoc.tn.cat$Date_.mm.dd.yyyy.,xaxt = "n",ylab="Dissolved OC (mg C/L)",xlab="Date (2021)",typ="n",ylim=c(0.5,3),cex.lab=2,cex.axis=1.5)
colnames(npoc.tn.combined)
?grep
df = npoc.tn.combined
View(df)
length(grep[pattern =  "NPOC_mg_C_per_L",x = colnames(df)],)
grep[pattern =  "NPOC_mg_C_per_L",x = colnames(df)]
grep[pattern = "NPOC_mg_C_per_L",x = colnames(df)]
colnames(df)
test = colnames(df)
grep[pattern = "NPOC",test]
"NPOC_mg_C_per_L"%in% colnames(df)
colnames(data.stats) = c("Sample_ID","Sample_name")
# Adding a column for just sample names, no reps
data.stats = as.data.frame(matrix(NA,ncol = 8, nrow = nrow(df)))
colnames(data.stats) = c("Sample_ID","Sample_name")
data.stats$Sample_ID = df$Sample_ID
View(data.stats)
data.stats$Sample_name = data.stats$Sample_ID
# Removing the replicates from the sample ID
data.stats$Sample_name = data.stats$Sample_ID
data.stats$Sample_name = gsub(pattern = "-1","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-2","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-3","",data.stats$Sample_name)
unique.sample.names = unique(data.stats$Sample_name)
# Adding a column for just sample names, no reps
data.stats = as.data.frame(matrix(NA,ncol = 8, nrow = nrow(df)))
colnames(data.stats) = c("Sample_ID","Sample_name","NPOC_CV","NPOC_variance","NPOC_range","TN_CV","TN_variance","TN_range")
data.stats$Sample_ID = df$Sample_ID
# Removing the replicates from the sample ID
data.stats$Sample_name = data.stats$Sample_ID
data.stats$Sample_name = gsub(pattern = "-1","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-2","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-3","",data.stats$Sample_name)
unique.sample.names = unique(data.stats$Sample_name)
# Calculate stats for each unique sample
for (i in 1:length(unique.sample.names)){
index = grep(unique.sample.names[i],data.stats$Sample_name)
npoc.temp = data.stats$NPOC_after_correcting_for_dilution[grep(unique.sample.names[i],data.stats$Sample_name)]
tn.temp = data.stats$TN_after_correcting_for_dilution[grep(unique.sample.names[i],data.stats$Sample_name)]
data.stats$NPOC_CV[index] = sd(npoc.temp) / mean(npoc.temp) * 100
data.stats$NPOC_range[index] =  max(npoc.temp) - min(npoc.temp)
data.stats$NPOC_variance[index] = var(npoc.temp)
data.stats$TN_CV[index] = sd(tn.temp) / mean(tn.temp) * 100
data.stats$TN_range[index] =  max(tn.temp) - min(tn.temp)
data.stats$TN_variance[index] = var(tn.temp)
}
# Calculate stats for each unique sample
for (i in 1:length(unique.sample.names)){
index = grep(unique.sample.names[i],data.stats$Sample_name)
npoc.temp = df$NPOC_after_correcting_for_dilution[grep(unique.sample.names[i],data.stats$Sample_name)]
tn.temp = df$TN_after_correcting_for_dilution[grep(unique.sample.names[i],data.stats$Sample_name)]
data.stats$NPOC_CV[index] = sd(npoc.temp) / mean(npoc.temp) * 100
data.stats$NPOC_range[index] =  max(npoc.temp) - min(npoc.temp)
data.stats$NPOC_variance[index] = var(npoc.temp)
data.stats$TN_CV[index] = sd(tn.temp) / mean(tn.temp) * 100
data.stats$TN_range[index] =  max(tn.temp) - min(tn.temp)
data.stats$TN_variance[index] = var(tn.temp)
}
View(data.stats)
colnames(df)-1
df = ion.compiled
names(df)[2:3]
ions = names(df)[2:ncol(df)]
length(ions)
data.stats = as.data.frame(matrix(NA,ncol = (length(ions)*3), nrow = nrow(df)))
#Creating column names
colnames(data.stats) = c("Sample_ID","Sample_name")
#Creating column names
name = c("Sample_ID","Sample_name")
c(name,a)
i = 1
a = paste0(ion[i],"_CV")
a = paste0(ions[i],"_CV")
c(name,a)
for (i in 1:length(ions)){
a = paste0(ions[i],"_CV")
b = paste0(ions[i],"_range")
c = paste0(ions[i],"_varience")
name = c(name,a,b,c)
}
data.stats = as.data.frame(matrix(NA,ncol = ((length(ions)*3)+2), nrow = nrow(df)))
colnames(data.stats) = name
data.stats$Sample_name = data.stats$Sample_ID
data.stats$Sample_name = gsub(pattern = "-1","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-2","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-3","",data.stats$Sample_name)
unique.sample.names = unique(data.stats$Sample_name)
data.stats$Sample_ID = df$Sample_ID
# Removing the replicates from the sample ID
data.stats$Sample_name = data.stats$Sample_ID
data.stats$Sample_name = gsub(pattern = "-1","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-2","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-3","",data.stats$Sample_name)
unique.sample.names = unique(data.stats$Sample_name)
grep(pattern = ion[j],x = df)
j = 1
grep(pattern = ion[j],x = df)
grep(pattern = ions[j],x = df)
grep(pattern = ions[j],x = colnames(df))
df[grep(pattern = ions[j],x = colnames(df))]
df1 = df[grep(pattern = ions[j],x = colnames(df))]
View(df1)
df1 = cbind.data.frame(df$Sample_ID,df[grep(pattern = ions[j],x = colnames(df))])
View(df1)
df1 = cbind.data.frame(Sample_ID = df$Sample_ID,df[grep(pattern = ions[j],x = colnames(df))])
View(df1)
j = 2
df1 = cbind.data.frame(Sample_ID = df$Sample_ID,df[grep(pattern = ions[j],x = colnames(df))])
i = 1
index = grep(unique.sample.names[i],data.stats$Sample_name)
df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
grep(pattern = paste0(ion[j],"_CV"),x = colnames(data.stats))
grep(pattern = paste0(ions[j],"_CV"),x = colnames(data.stats))
data.stats[index,grep(pattern = paste0(ions[j],"_CV"),x = colnames(data.stats))] = sd(temp) / mean(temp) * 100
temp = df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
data.stats[index,grep(pattern = paste0(ions[j],"_CV"),x = colnames(data.stats))] = sd(temp) / mean(temp) * 100
View(data.stats)
for (j in 2:length(ions)){
df1 = cbind.data.frame(Sample_ID = df$Sample_ID,df[grep(pattern = ions[j],x = colnames(df))])
for (i in 1:length(unique.sample.names)){
index = grep(unique.sample.names[i],data.stats$Sample_name)
temp = df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
data.stats[index,grep(pattern = paste0(ions[j],"_CV"),x = colnames(data.stats))] = sd(temp) / mean(temp) * 100
data.stats[index,grep(pattern = paste0(ions[j],"_range"),x = colnames(data.stats))] =  max(temp) - min(temp)
data.stats[index,grep(pattern = paste0(ions[j],"_variance"),x = colnames(data.stats))] = var(temp)
}
}
View(data.stats)
ions = names(df)[2:ncol(df)]
data.stats = as.data.frame(matrix(NA,ncol = ((length(ions)*3)+2), nrow = nrow(df)))
#Creating column names
name = c("Sample_ID","Sample_name")
for (i in 1:length(ions)){
a = paste0(ions[i],"_CV")
b = paste0(ions[i],"_range")
c = paste0(ions[i],"_variance")
name = c(name,a,b,c)
}
colnames(data.stats) = name
data.stats$Sample_ID = df$Sample_ID
# Removing the replicates from the sample ID
data.stats$Sample_name = data.stats$Sample_ID
data.stats$Sample_name = gsub(pattern = "-1","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-2","",data.stats$Sample_name)
data.stats$Sample_name = gsub(pattern = "-3","",data.stats$Sample_name)
unique.sample.names = unique(data.stats$Sample_name)
for (j in 2:length(ions)){
df1 = cbind.data.frame(Sample_ID = df$Sample_ID,df[grep(pattern = ions[j],x = colnames(df))])
for (i in 1:length(unique.sample.names)){
index = grep(unique.sample.names[i],data.stats$Sample_name)
temp = df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
data.stats[index,grep(pattern = paste0(ions[j],"_CV"),x = colnames(data.stats))] = sd(temp) / mean(temp) * 100
data.stats[index,grep(pattern = paste0(ions[j],"_range"),x = colnames(data.stats))] =  max(temp) - min(temp)
data.stats[index,grep(pattern = paste0(ions[j],"_variance"),x = colnames(data.stats))] = var(temp)
}
}
View(data.stats)
View(df)
temp
temp = df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
i = 1
temp = df1[grep(unique.sample.names[i],data.stats$Sample_name),2]
tenp
temp
View(df)
